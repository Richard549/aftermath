\input texinfo   @c -*-texinfo-*-
@setfilename openmp-shortguide.info
@comment %**start of header
@documentencoding UTF-8
@dircategory Software development
@direntry
      * Aftermath: (aftermath).                Aftermath
@end direntry
@afourpaper
@include version.texi
@settitle A Short Guide for Analyzing OpenMP Traces with Aftermath
@syncodeindex pg cp
@comment %**end of header
@copying
This is a short guide for analyzing OpenMP traces with Aftermath.

Copyright @copyright{} 2016 Andi Drebes

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3
published by the Free Software Foundation; with no Invariant Sections,
with no Front-Cover Texts, and with no Back-Cover Texts.  A copy of
the license is included in the section entitled ``GNU Free
Documentation License''.
@end copying

@titlepage
@title A Short Guide for Analyzing OpenMP Traces with Aftermath
       @author Andi Drebes (@email{andi@@drebesium.org})
@page
@vskip 0pt plus 1filll
@insertcopying
@end titlepage

@contents

@node Analyzing OpenMP traces
@chapter Analyzing OpenMP Traces
This short guide provides a quick overview of how to analyze traces
generated by OpenMP programs using our tool for trace-based analysis
of parallel programs, called @dfn{Aftermath}. This involves the
Aftermath tool itself, for visualizing and analyzing traces, and the
Aftermath OpenMP run-time for generating trace files from OpenMP
programs. We first show what a user can expect from both tools by
summarizing the status of the OpenMP support in Aftermath and
Aftermath-OpenMP. After the instructions for the configuration and
installation of Aftermath and Aftermath-OpenMP and a short summary of
Aftermath's terminology for OpenMP, we present short, self-contained
examples for the analysis of OpenMP programs.

@section Status of the OpenMP Support

Aftermath has originally been designed for the analysis of dependent
tasks in OpenStream. However, as many of the analyses for OpenStream
programs are useful for programs from different parallel frameworks,
in particular OpenMP, Aftermath has recently been extended to support
traces generated by OpenMP programs. Complementary to the OpenMP
support in Aftermath, we provide Aftermath-OpenMP, an instrumented
OpenMP run-time library that collects performance data at execution
time and that generates a trace file at program termination. This
freely available run-time is based on the LLVM/clang OpenMP run-time,
which in turn is based on the Intel OpenMP run-time. OpenMP programs
executed with Aftermath-OpenMP and analyzed with Aftermath may use any
construct implemented by the LLVM/clang OpenMP run-time, but tracing
is restricted to the following features:

@itemize
@item
Parallel loops can have a static, dynamic or guided schedule and may
be chunked (explicitly or implicitly). The iteration space can be
signed or unsigned, but induction variables may have at most 64
bits. Currently, only single loops are supported, multiple loops
combined with a collapse clause cannot be traced properly.

@item
The support for tasks is currently limited to independent
tasks. Dependent tasks may be used, but the run-time system does not
capture data dependencies and Aftermath can only display and analyze
independent tasks.

@item
Parallel loops and tasks may be mixed without restrictions. Parallel
regions may be nested.

@item
As Aftermath-OpenMP captures data on a per-core basis, worker threads
must be pinned to cores to avoid that the same worker traces data for
multiple cores. In addition, the number of workers must be less than
or equal to the number of available cores to avoid that two or more
workers trace data for the same core.

@item
The Aftermath-OpenMP run-time requires that programs must be compiled
with @command{clang} version 3.8.0 or higher. Any level of
optimization by the compiler is allowed, but debugging symbols must be
available in the final executable.
@end itemize

@noindent
The latter restrictions regarding the mapping of workers to cores, the
supported versions of LLVM/clang and the presence of debugging symbols
are automatically verified and enforced by the
@command{aftermath-openmp-trace} command distributed with the
Aftermath-OpenMP run-time.

@section Installing Aftermath
Aftermath is distributed with the git version control system and can
be cloned from our public git repository at
@url{git://git.drebesium.org}:

@smallexample
$ git clone git://git.drebesium.org/aftermath.git
@end smallexample

@noindent
The installation and configuration of Aftermath is done using the GNU
Autotools. The @file{bootstrap} script provided by the Aftermath
sources invokes the Autotools in the right order and with the
appropriate parameters:

@smallexample
$ cd aftermath
$ ./bootstrap
@end smallexample

@noindent
This initial bootstrapping step generates the @file{configure} script,
which checks for the presence of software packages required by
Aftermath and that configures the source files. This script is invoked
by executing:

@smallexample
$ ./configure
@end smallexample

@noindent
If all of the required packages are present, @file{configure}
generates a make file with rules to build and install the
Aftermath. The rules for the compilation and installation are
interpreted by the @command{make} utility:

@smallexample
$ make
@end smallexample

@noindent
To speed up compilation on multicore systems, you may use the
@command{-j} option of @command{make}. For example, if your machine
has four processors, you can instruct @command{make} to run up to four
compilation jobs simultaneously by running @command{make -j4}. The
installation step usually requires super user rights and must be ran
directly from a super user shell, e.g., from a shell spawned by the
@command{su} command, or indirectly using a program capable of
elevating rights, such as @command{sudo}:

@smallexample
$ su
# make install
@end smallexample

@noindent
@emph{or}

@smallexample
$ sudo make install
@end smallexample

@noindent
If you do not want to install Aftermath system-wide or if you want to
install the program as a non-privileged user, you can specify an
alternate prefix during the configuration by invoking
@command{configure} with the @command{--prefix} argument. The
following sequence of commands installs Aftermath into a folder named
@file{extra} within your home directory:
@smallexample
$ ./configure --prefix=$HOME/extra
$ make
$ make install
@end smallexample

We strongly recommend that you run the unit tests provided by the
Aftermath sources. The @command{check} target of the previously
generated make file executes these tests automatically:

@smallexample
$ make check
@end smallexample

@noindent
In addition, you may also run the tests with the @command{valgrind}
utility (if available on your system) by invoking the
@command{valgrind-check} target:

@smallexample
$ make valgrind-check
@end smallexample


@section Installing the Aftermath OpenMP Run-time
In order to generate traces from an OpenMP program, it is necessary to
install the instrumented Aftermath OpenMP run-time. The sources of the
run-time are available from another repository on the same public git
server. Start by cloning the repository with @command{git}:

@smallexample
$ git clone git://git.drebesium.org/aftermath-openmp.git
@end smallexample

@noindent
Next, configure the run-time using the @command{cmake} command. If you
have not used any custom prefix during the configuration of Aftermath,
it is sufficient to run @command{cmake} without any additional
parameter:

@smallexample
$ cd aftermath-openmp
$ cmake .
@end smallexample

@noindent
Otherwise, you need to indicate where the Aftermath libraries and
include files can be found. This can be done using the
@var{CMAKE_LIBRARY_PATH} and @var{CMAKE_INCLUDE_PATH} variables. You
may also indicate a prefix for the installation of the Aftermath
OpenMP run-time through the @var{CMAKE_INSTALL_PREFIX:PATH} variable.

@smallexample
$ cd aftermath-openmp
$ cmake -DCMAKE_LIBRARY_PATH=/path/to/aftermath-installation/lib \
        -DCMAKE_INCLUDE_PATH=/path/to/aftermath-installation/include \
        -DCMAKE_INSTALL_PREFIX:PATH=/where/to/install/the/run-time \
        .
@end smallexample

@noindent
Note that there is a dot at the end of the @command{cmake}
commands. To build and install the run-time, simply run:

@smallexample
$ make
$ make install
@end smallexample

@noindent
You are now ready to generate traces with the instrumented run-time by
using the @command{aftermath-openmp-trace} command. Remember that any
program must be compiled with a recent version of @command{clang} (>=
3.8.0) and that debugging symbols must be embedded into the executable
using the @command{-g} switch. You may check if the installation is
correct by running the tests provided in the @file{aftermath-tests}
folder:

@smallexample
$ cd aftermath-tests
$ make all-traces
@end smallexample

@noindent
This generates a series of traces in @file{aftermath-tests/traces}
that can be opened with Aftermath, e.g.:

@smallexample
$ aftermath traces/for-static.ost
@end smallexample


@section Terminology and Concepts

In the analysis of loops and tasks, we will use the following terms in
addition to the terminology of the OpenMP specification:
@dfn{iteration set}, @dfn{iteration period}, and @dfn{task period}. We
further refer to dynamic instances of loops and tasks simply as
@dfn{loops} and @dfn{tasks}. These terms are usually associated with
constructs in the source code, but overloading them allows for a
simple terminology for dynamic analysis and remains unambiguous within
the respective context. If a distinction with constructs and instances
is necessary, we use the terms @dfn{loop construct} and @dfn{task
construct}.

@subsection Iteration sets
We define an iteration set as a (not necessarily contiguous) portion
of the iteration space of a parallel loop. This lets us define the
iterations assigned to a worker in a generic way, independently from
the loop's schedule and chunking. To illustrate this term, consider
the following example with a single parallel loop, executed by four
worker threads:

@smallexample
set_omp_num_threads(4);

#pragma omp parallel for schedule(static, 10)
for(int i = 0; i < 100; i++)
  do_something(i);
@end smallexample

@noindent
The static schedule and the chunk size of 10 cause chunks of ten
iterations to be assigned to the four workers in a round-robin
fashion. That is, the first worker executes iterations 0--9, 40--49,
and 80--89, the second worker executes iterations 10--19, 50--59,
90-99, and so on. The iteration set defining all iterations executed
by the first worker is thus
@iftex
@math{@{0, @ldots, 9@} @cup @{40, @ldots, 49@} @cup @{80, @ldots, 89@}}.
@end iftex
@ifnottex
@math{@{0, …, 9@} ∪ @{40, …, 49@} ∪ @{80, …, 89@}}.
@end ifnottex
Aftermath displays iteration sets as
lists of intervals, e.g., [0, 9], [40, 49], [80, 89].

@subsection Iteration periods and task periods

The execution of an iteration set can be interrupted, e.g., if the
worker hits a nested barrier within a parallel loop. This is the case
in the following example between the calls to @code{do_something} and
@code{do_something_else}:

@smallexample
#pragma omp parallel for schedule(static)
for(int i = 0; i < 100; i++) @{
  do_something(i);

  #pragma omp task
  a_task();

  #pragma omp task
  another_task();

  #pragma omp taskwait

  do_something_else(i);
@}
@end smallexample

We refer to contiguous periods of execution of an iteration set as an
iteration period. As the execution of an iteration set might be
interrupted multiple times, multiple iteration periods might be
associated to a single iteration set. In the example above each
iteration set has two iteration periods: the first iteration period
contains the call to @code{do_something} and the creation of the two
tasks and ends at the taskwait barrier, while the second period
corresponds to the code executed after the barrier, with the call to
@code{do_something_else}.

Note that information about the progress of the execution of an
iteration set is generally not captured by the run-time system. Hence,
Aftermath can only determine which iterations belong to an iteration
set, but it cannot determine which iterations have been executed
during a specific iteration period of an iteration set. For example,
when executing the code of the following listing, the run-time does
not necessarily capture that the first iteration period of each
iteration set comprises the first three iterations and that the second
iteration period is composed of the last seven iterations:

@smallexample
#pragma omp parallel for schedule(static, 10)
for(int i = 0; i < 100; i++) @{
  do_something(i);

  if(i % 10 == 3) @{
    #pragma omp task
    a_task();

    #pragma omp taskwait
  @}

  do_something_else(i);
@}
@end smallexample

Similar to iteration sets task execution may be interrupted by
barriers. We thus define a task period as a contiguous periods of
execution of a task.

@section Analyzing Parallel Loops
In the following analyses of OpenMP programs, we consider different
implementations of a program that calculates the amount of prime
numbers in an interval using a naive prime test. We start the analysis
with the implementation below, based on a statically scheduled loop:

@example
#include <stdio.h>
#include <math.h>

int isprime_naive(int n)
@{
  if(n % 2 == 0 && n != 2)
    return 0;

  for(int j = 3; j <= sqrt(n); j += 2)
    if(n % j == 0)
      return 0;

  return 1;
@}

int main(int argc, char** argv)
@{
  int n = 1;

  #pragma omp parallel
  @{
    #pragma omp for schedule(static) reduction(+:n)
    for(int i = 3; i < 1000000; i += 2)
      n += isprime_naive(i);
  @}

  printf("There are %d prime numbers in the interval\n", n);

  return 0;
@}
@end example

Paste the listing above to a file named @file{prime_naive.c} or copy
the file from the @file{doc/examples} folder of the Aftermath source
tree and compile the program using @command{clang} version 3.8.0 (or
later) with debugging symbols:

@smallexample
$ clang -fopenmp -g -o prime_naive prime_naive.c -lm
@end smallexample

@noindent
Next, generate a trace file using the @command{aftermath-openmp-trace}
program from the Aftermath-OpenMP run-time and open the trace file in
Aftermath:

@smallexample
$ aftermath-openmp-trace -o prime_naive.ost -f -- ./prime_naive
$ aftermath prime_naive.ost
@end smallexample

After starting Aftermath, you will notice that the time line appears
to be empty. This is because on startup the time line is in so-called
@i{state mode}, indicating the different run-time and application
states each worker traverses over time. The Aftermath OpenMP run-time
traces states for barriers, critical regions, single and master
constructs, but does associate a state to parallel loops. The reason
for the absence of a loop state is that states only capture very
limited information (an interval), whereas more detailed information
is required for the analysis of parallel loops. Therefore, Aftermath
provides specific time line modes for loops, which do not only provide
a quick visual overview of loop executions, but that also let the user
select portions of loop executions to obtain accurate information
about loop characteristics. There are four loop-specific time line
modes:

@itemize
@item
The @emph{loop construct mode} assigns a different color to each loop
construct and visualizes iteration intervals with the color associated
to their loops' constructs.
@item
In @emph{loop mode} Aftermath assigns a different color to each
instance of a loop. That is, if the same loop construct is executed
twice, the intervals associated to each execution are visualized using
a different color.
@item
The @emph{iteration set mode} uses a different color for each
iteration set. This means that all iteration periods of the same
iteration set are visualized using the same color.
@item
In @emph{iteration period mode} the tool associates a different color
to each iteration period, such that the iteration periods of a given
iteration set can be distinguished visually on the time line.
@end itemize

@float Figure,fig:toolbar:openmp-modes
@center @image{images/aftermath-toolbar-loop-modes,,3cm,,png}
@caption{Selection of the time line mode for the visualization of parallel loops}
@end float

@float Figure,fig:prime_naive:loop-modes
@multitable @columnfractions .1 .4 .4 .1
@item
@tab @image{images/aftermath-omp-prime_naive-tlzoom-constructs,,2.3cm,,png}
@tab @image{images/aftermath-omp-prime_naive-tlzoom-itersets,,2.3cm,,png}
@tab
@end multitable

@caption{The time line in loop construct mode (left) and iteration set
mode (right)}
@end float

Loop-specific modes can be selected by using the drop down menu next
to the OpenMP button in the tool bar, as shown in
@ref{fig:toolbar:openmp-modes}. @ref{fig:prime_naive:loop-modes} shows
the time line in loop construct mode (left) and iteration set mode
(right) for the @command{prime_naive} application. As the loop in the
application is executed only once, the trace contains only one loop
per loop construct. Hence, the loop mode yields the same visualization
as the loop construct mode and is therefore omitted in this
document. Similarly, the iteration set mode is identical to the
iteration period mode, since the execution of an iteration set is
never interrupted, such that there is exactly one iteration period per
iteration set. As all workers are involved in the schedule and as all
of them execute the same loop, the loop construct mode shows the same
color for all workers. The iteration set mode displays a different
color for each worker, since each worker is provided with its own part
of the iteration space and thus its own iteration set.

Both visualizations immediately reveal that there is a strong
imbalance between workers. This is the result of an inappropriate use
of the static schedule: each worker executes the same number of
iterations, but the amount of work grows with each iteration, leaving
the workers with very different workloads. The partitioning of the
iteration space and the amount of work can be inspected by clicking on
an iteration period on the time line.

The contents of the OpenMP loop tab in the detailed text view below
the time line for each worker is shown in @ref{fig:loop-details}. As
expected, the frames for loop constructs and for loops show the same
information for all workers. The loop construct frame states that each
worker has executed the same loop (``For addr: 0x400d60''), with a
static schedule (``Schedule: static'') and without a specified chunk
size (``Chunked: No''). The loop frame indicates that the iteration
space is represented by the interval @math{[0, 499998]} with an
increment of one. This does not reflect the original specification in
the source code, which indicated the interval @math{[3, 999999]} with
an increment of two. However, the trace file is generated at execution
time and thus after all transformations by the compiler. Hence, the
iteration space and increment shown in Aftermath reflect the
transformed loop characteristics. In addition to this information, the
tab also indicates the number of workers involved in the execution of
the loop (``#Workers: 4''), the number of chunks (``#Chunks: 4''), the
number of iteration sets (``#Iteration sets: 4''), the wall clock time
from the beginning of the first iteration period to the end of the
last iteration period (``Wall clock time: 481.70 Mcycles''), the
total, aggregated time spent by all workers (``Total time: 1.45
Gcycles'') as well as the chunk load balance and the parallelism
efficiency. The chunk load imbalance is calculated as follows:

@tex
$$B_{\rm itset} = 1 - {{\rm max\{t_{\rm tot}^{\rm itset}\}} - {\rm avg\{t_{\rm tot}^{\rm itset}\}} \over {\rm max\{t_{\rm tot}^{\rm itset}\}}}$$
@end tex
@html
<center>
B<sub>itset</sub> = 1 - (max{t<sub>tot</sub><sup>itset</sup>} - avg{t<sub>tot</sub><sup>itset</sup>}) / max{t<sub>tot</sub><sup>itset</sup>}
@end html
where
@tex
${\rm max\{t_{\rm tot}^{\rm itset}\}}$
@end tex
@html
max{t<sub>tot</sub><sup>itset</sup>} 
@end html
and
@tex
${\rm avg\{t_{\rm tot}^{\rm itset}\}}$
@end tex
@html
avg{t<sub>tot</sub><sup>itset</sup>} 
@end html
are the maximum and average total durations of the iteration sets,
respectively. A value close to one indicates that there is little
difference between those values and that the partitioning of work
leads to iteration sets with similar durations. A value lower than one
indicates an imbalance between iteration sets. The parallelism
efficiency is defined as:

@tex
$$E_{\rm par} = {t_{\rm tot}^{\rm loop} \over t_{\rm wct}^{\rm loop} \cdot N_{\rm cpus}}$$
@end tex
@html
E<sub>par</sub> = t<sub>tot</sub><sup>loop</sup> / (t<sub>wct</sub><sup>loop</sup> &middot; N<sub>cpus</sub>)
</center>
@end html
with
@tex
$t_{\rm tot}^{\rm loop}$
@end tex
@html
t<sub>tot</sub><sup>loop</sup> 
@end html
being the total, aggregated time spent in the loop by all workers,
@tex
$t_{\rm wct}^{\rm loop}$
@end tex
@html
t<sub>wct</sub><sup>loop</sup> 
@end html
being the wall clock time from the start of the loop to its end, and
@tex
$N_{\rm cpus}$
@end tex
@html
N<sub>cpus</sub> 
@end html
being the number of cores involved in the execution. A value close to
one indicates that the total amount of work of the loop could be
distributed uniformly among the workers, while a value closer to zero
indicates that the loop needed more time to execute that the total
amount of work divided by the number of workers, indicating a load
imbalance. Note that the chunk load balance and the parallelism
efficiency can differ substantially. For example, if a loop contains a
barrier, the chunk balance can be high, while the parallelism
efficiency of the loop might be low.

The iteration set frames show different values for each worker: the
first worker executes iterations 0 to 124999, the second worker
executes iterations 125000 to 249999, the third worker executes
iterations 250000 to 374999, and the last worker is responsible for
iterations 375000 to 499998. As expected, each iteration set is
composed of a single iteration period (``#Iteration periods: 1''). The
time for each iteration period varies between 231.05 million cycles
and 475.09 million cycles.

Finally, the iteration period frames show the exact timestamps of the
beginning and end of each iteration period. A click on a timestamp
centers the time line at the respective position.

@float Figure,fig:loop-details

@multitable @columnfractions .5 .5
@item @image{images/aftermath-omp-prime_naive-cpu0,,2.3cm,,png}
@tab @image{images/aftermath-omp-prime_naive-cpu1,,2.3cm,,png}
@item @image{images/aftermath-omp-prime_naive-cpu2,,2.3cm,,png}
@tab @image{images/aftermath-omp-prime_naive-cpu3,,2.3cm,,png}
@end multitable

@caption{Detailed information for core 0 to core 3 (top left to bottom right)}
@end float

Let us now investigate the influence of the schedule of the parallel
loop on the load imbalance between workers, starting with a dynamic
schedule. As the loop in the example performs several hundred thousand
iterations, the default chunk size of 1 for dynamic schedules is
likely to cause significant synchronization overhead. To reduce this
overhead, we will use a chunk size of 1,000. To apply the new schedule
and chunk size, copy the previous code to a new file named
@file{prime_naive_dynamic.c} and replace the pragma above the loop
with the following line (or use the file with the same name in the
@file{doc/texi} folder of the Aftermath source code):

@smallexample
#pragma omp for schedule(dynamic, 1000) reduction(+:n)
@end smallexample

@noindent
Then, compile the new program, create a new trace file named
@file{prime_naive_dynamic.ost} and open the trace with Aftermath:

@smallexample
$ clang -fopenmp -g -o prime_naive_dynamic prime_naive_dynamic.c -lm
$ aftermath-openmp-trace -o prime_naive_dynamic.ost -f -- ./prime_naive_dynamic
$ aftermath prime_naive_dynamic.ost
@end smallexample


@ref{fig:prime_naive_dynamic:loop-modes} shows the time line for the
resulting trace in loop construct mode and iteration set mode. Both
modes show that there is significantly less imbalance between
workers. This is the result of the dynamic assignment of many small
iteration sets, as illustrated by the numerous intervals with
different colors in iteration set mode. The detailed text view for the
dynamic schedule in @ref{fig:prime_naive_dynamic:detail} confirms that
the wall clock execution time of the loop could be reduced to about
389 million cycles. The parallelism efficiency has been increased
significantly to almost 100%. The chunking leads to iteration sets
with a highly different execution time, which results in a reduced
chunk load balance of less than 10%.

@float Figure,fig:prime_naive_dynamic:loop-modes
@multitable @columnfractions .1 .4 .4 .1
@item
@tab @image{images/aftermath-omp-prime_naive_dynamic-tlzoom-constructs,,2.3cm,,png}
@tab @image{images/aftermath-omp-prime_naive_dynamic-tlzoom-itersets,,2.3cm,,png}
@tab
@end multitable
@caption{The time line in loop construct mode (left) and iteration set
mode (right) for the dynamic schedule}
@end float

@float Figure,fig:prime_naive_dynamic:detail
@center @image{images/aftermath-omp-prime_naive_dynamic-detail,,3cm,,png}
@caption{Detailed text view for the dynamic schedule}
@end float

Let us now investigate how the @emph{guided} schedule behaves. This
schedule assigns larger iteration sets to the workers at the beginning
of the execution of the loop and decreases the size towards its
end. This allows the run-time to compensate a load imbalance caused by
an increasing amount of work per iteration, just like the workload of
the example. For this last version of the prime number test, copy the
code to another file named @file{prime_naive_guided.c} and replace the
loop construct with:

@smallexample
#pragma omp for schedule(guided) reduction(+:n)
@end smallexample

@noindent
or copy the file with the same name from the @file{doc/examples}
directory. Compile and execute the program in order to generate a new
trace file, @file{prime_naive_guided.ost}, and open the generated
trace with Aftermath:

@smallexample
$ clang -fopenmp -g -o prime_naive_guided prime_naive_guided.c -lm
$ aftermath-openmp-trace -o prime_naive_guided.ost -f -- ./prime_naive_guided
$ aftermath prime_naive_guided.ost
@end smallexample

@noindent
@ref{fig:prime_naive_guided:loop-modes} shows again the time line in
loop construct mode and iteration set mode. The work is now almost
perfectly balanced between workers, with a parallel efficiency of
99.995%, shown in @ref{fig:prime_naive_guided:detail}. The time line
in iteration set mode also indicates that the the increasing amount of
work per iteration is in fact overcompensated by the decreasing size
of iteration sets, since the duration for the execution of each set
decreases.

@float Figure,fig:prime_naive_guided:loop-modes
@multitable @columnfractions .08 .42 .42 .08
@item
@tab @image{images/aftermath-omp-prime_naive_guided-tlzoom-constructs,,2.3cm,,png}
@tab @image{images/aftermath-omp-prime_naive_guided-tlzoom-itersets,,2.3cm,,png}
@tab
@end multitable
@caption{The time line in loop construct mode (left) and iteration set
mode (right) for the guided schedule}
@end float

@float Figure,fig:prime_naive_guided:detail
@center @image{images/aftermath-omp-prime_naive_guided-detail,,3cm,,png}
@caption{Detailed text view for the guided schedule}
@end float

@section Analyzing Tasks
For our next example, illustrating the analysis of tasks, consider the
code below, representing a task-based version of the program
determining the amount of prime numbers in an interval:

@example
#include <stdio.h>
#include <math.h>

static inline int imax(int a, int b)
@{
  return (a > b) ? a : b;
@}

int isprime_naive(int n)
@{
  if(n % 2 == 0 && n != 2)
    return 0;

  for(int j = 3; j <= sqrt(n); j += 2)
    if(n % j == 0)
      return 0;

  return 1;
@}

int main(int argc, char** argv)
@{
  int n = 1;
  int slices = 100;
  int nloc[slices];
  int max = 1000000;
  int slice_sz = max / slices;

  #pragma omp parallel
  @{
    #pragma omp for schedule(static)
    for(int i = 0; i < slices; i++) @{
      nloc[i] = 0;

      #pragma omp task
      for(int j = imax(i*slice_sz, 3); j < (i+1)*slice_sz; j++)
        nloc[i] += isprime_naive(j);
    @}

    #pragma omp taskwait

    #pragma omp for schedule(static) reduction(+:n)
    for(int i = 0; i < slices; i++)
      n += nloc[i];
  @}

  printf("There are %d prime numbers in the interval\n", n);

  return 0;
@}
@end example

The program divides the interval from 0 to 1,000,000 into 100
equal-sized slices and creates a task for each of the slices in a
parallel loop. Each task calculates the amount of prime numbers in its
associated slice and writes the result at the respective position to
an array named @var{nloc}. After a barrier waiting for the termination
of all generated tasks, a second parallel loop sums up the results of
@var{nloc} and writes the final result to @var{n}. In a last step, the
master thread prints the result after the end of the parallel region.

Paste the code to a new file named @file{prime_naive_tasks.c} or copy
the file from @file{doc/examples}, compile it, generate a trace file
and load the trace with Aftermath as usual:

@smallexample
$ clang -fopenmp -g -o prime_naive_tasks prime_naive_tasks.c -lm
$ aftermath-openmp-trace -o prime_naive_tasks.ost -f -- ./prime_naive_tasks
$ aftermath prime_naive_tasks.ost
@end smallexample

@ref{fig:prime_naive_tasks:task-modes} shows the time line in task
construct mode and task mode. The first observation on the time line
in task mode is that task execution covers almost the entire duration
of the program. Only a small part towards the end of the execution is
not spent on task execution for some of the workers. As there is only
one task construct, only one color is used. The many different colors
in task mode on the right side of the figure represent the task
instances. @footnote{Although there are a hundred tasks present in the
trace, less than a hundred colors are used in the figure. This is
because the number of distinct colors in Aftermath is limited, causing
the tool to use the same color for different tasks. If necessary, the
color of each task construct, task or task period can be changed
manually.}

@float Figure,fig:prime_naive_tasks:task-modes
@multitable @columnfractions .08 .42 .42 .08
@item
@tab @image{images/aftermath-omp-prime_naive_tasks-tlzoom-constructs,,2.3cm,,png}
@tab @image{images/aftermath-omp-prime_naive_tasks-tlzoom-tasks,,2.3cm,,png}
@tab
@end multitable
@caption{The time line in task construct mode (left) and task mode
(right) for the implementation with tasks}
@end float

Each task can be inspected with a click on the associated task period
on the time line. @ref{fig:prime_naive_tasks:detail} shows the
detailed text view for one of the tasks. Similar to the detailed text
view for parallel loops, the view is split into three parts, providing
information about the task construct, the task and the selected task
period. The task construct frame confirms that there are exactly 100
instances of the task (``#Instances: 100''). As the tasks do not
contain barriers and thus cannot be interrupted, there is only one
task period per task, as shown in the task frame (``#Task periods:
1''). The executing core, the start and end timestamp as well as the
exact duration of the task period are given in the rightmost frame.

@float Figure,fig:prime_naive_tasks:detail
@center @image{images/aftermath-omp-prime_naive_tasks-detail,,2cm,,png}
@caption{Detailed text view for the implementation with tasks}
@end float

@ref{fig:prime_naive_tasks:states} shows the time line in state mode
for the entire execution of the program (left) and zoomed on the end
of the execution (right). The mapping between colors and states are
given in a table at the bottom of the statistics tab. The table for
the example, given in @ref{fig:prime_naive_tasks:stats}, shows five
states:

@itemize
@item
@dfn{barrier} (light blue), indicating time spent in an implicit
barrier of a parallel loop or in explicit barriers.

@item
@dfn{critical} (dark blue), @dfn{single} (white), and @dfn{master}
(pink), associated to time spent in a critical region, in a single
construct, and in a master construct, respectively.

@item
@dfn{taskwait} (red), representing time spent on synchronization with
tasks, either at the end of a task or in a taskwait barrier.
@end itemize

The time line shows short red intervals associated to task
synchronization throughout the entire execution (at the end of each
task) and longer red intervals for the taskwait barrier towards the
end of the execution of the program. The intervals at the end of the
execution are complementary to the slight imbalance between workers in
the task mode in @ref{fig:prime_naive_tasks:task-modes}, shown
earlier. A zoom in this part, shown on the right side of Figure
@ref{fig:prime_naive_tasks:states}, also indicates that some time was
spent in the implicit barrier at the end of the second parallel
loop. Exact statistics on how much time was spent in the different
states can be obtained by clicking on the button labeled @emph{Select
from graph} in the statistics panel and by selecting an interval from
the time line. The table showing the mapping between colors and states
is then immediately updated with two values per state. The first value
indicates the fraction of the selected interval that was spent in each
state, while the second value indicates how many workers on average
were in this state simultaneously. The values in
@ref{fig:prime_naive_tasks:stats} are very low: only about 1.5% and
0.01% of the time were spent on task synchronization and barriers,
respectively. The rest of the time was spent on the execution of tasks
and loops.

@float Figure,fig:prime_naive_tasks:states
@multitable @columnfractions .08 .42 .42 .08
@item
@tab @image{images/aftermath-omp-prime_naive_tasks-tlzoom-states,,2.3cm,,png}
@tab @image{images/aftermath-omp-prime_naive_tasks-tlzoom-states-end,,2.3cm,,png}
@tab
@end multitable
@caption{The time line in state mode (left: entire execution of the
program; right: zoom on the end)}
@end float

@float Figure,fig:prime_naive_tasks:stats
@center @image{images/aftermath-omp-prime_naive_tasks-stats,,2cm,,png}
@caption{Statistics about the time spent in the different states}
@end float

@section Conclusion and Perspectives
In this short guide, we showed how to install Aftermath and
Aftermath-OpenMP and provided a few scenarios for the Analysis of
parallel loops and tasks. We investigated the influence of the loop
schedule and the use of tasks on a synthetic program, calculating the
amount of prime numbers in the interval from 0 to 1,000,000. We showed
how to inspect the partitioning of the iteration space of loops and
how to locate imbalances between workers. We also pointed out how to
quantify the time spent in different run-time states, in particular
barriers for loops and tasks.

Aftermath constantly evolves and more advanced techniques for the
analysis of OpenMP programs will be integrated. Aftermath and
Aftermath-OpenMP are released under free software licenses. Your bug
reports, suggestions for improvements and code contributions are
welcome. To find out more on how to contribute and to stay informed,
please visit our website at
@url{http://www.openstream.info/aftermath}.

@include fdl.texi

@bye
